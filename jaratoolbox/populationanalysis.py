"""
Classes for analyzing the activity of simultaneously recorded neurons.
"""

import numpy as np
from jaratoolbox import ephyscore
from jaratoolbox import spikesanalysis


class Population():
    def __init__(self, celldb):
        """
        Args:
            celldb (pandas.DataFrame): data frame generated by celldatabase.generate_cell_database()
                                       all cells must be simultaneously recorded.
        """
        self.celldb = celldb
        self.clusters = self.celldb.cluster.unique()
        self.tempCell = None   # Container for cell template
        self.ephysData = None
        self.behavData = None
        self.nTrials = None
        self.spikeTimes = None
        self.events = None
        self.clusterEachSpike = None
        self.spikeTimesFromEventOnset = [] # One array per cluster
        self.trialIndexForEachSpike = []   # One array per cluster
        self.indexLimitsEachTrial = []   # One array per cluster
        self.spikeCountMat = []
        # -- Make a dictionary (look-up table) of clusterID:indc --
        self.clusterIDtoIndex = {k:v for v,k in enumerate(self.clusters)}
        
    def load(self, sessiontype):
        self.tempCell = ephyscore.Cell(self.celldb.iloc[0])
        self.tempCell.cluster = None
        self.ephysData, self.behavData = self.tempCell.load(sessiontype)
        self.spikeTimes = self.ephysData['spikeTimes']
        self.events = self.ephysData['events']
        self.clusterEachSpike = self.ephysData['clusterEachSpike']
        
    def get_spiketimes(self, cluster):
        spikesThisCluster = self.clusterEachSpike==cluster
        return self.spikeTimes[spikesThisCluster]
       
    def eventlocked_spiketimes(self, eventOnsetTimes, timeRange):
        for indc, clusterID in enumerate(self.clusters):
            spikeTimesThisCluster = self.get_spiketimes(clusterID)
            (spikeTimesFromEventOnset,trialIndexForEachSpike,indexLimitsEachTrial) = \
                spikesanalysis.eventlocked_spiketimes(spikeTimesThisCluster, eventOnsetTimes, timeRange)
            self.spikeTimesFromEventOnset.append(spikeTimesFromEventOnset)
            self.trialIndexForEachSpike.append(trialIndexForEachSpike)
            self.indexLimitsEachTrial.append(indexLimitsEachTrial)
        self.nTrials = len(eventOnsetTimes)

    def get_locked_spiketimes(self, cluster):
        indc = self.clusterIDtoIndex[cluster]
        return (self.spikeTimesFromEventOnset[indc],
                self.trialIndexForEachSpike[indc],
                self.indexLimitsEachTrial[indc])

    def spiketime_to_spikecounts(self, binEdges):
        if not len(self.spikeTimesFromEventOnset):
            print('You first need to run self.eventlocked_spiketimes()')
            return
        else:
            nBins = len(binEdges)-1
            self.spikeCountMat = np.empty((len(self.clusters), self.nTrials, nBins))
            for indc, clusterID in enumerate(self.clusters):
                spikeCountMat = spikesanalysis.spiketimes_to_spikecounts(self.spikeTimesFromEventOnset[indc],
                                                                         self.indexLimitsEachTrial[indc],
                                                                         binEdges)
                self.spikeCountMat[indc, :, :] = spikeCountMat

    
    def cell_name(self, cluster):
        objStr = '{} {} {:0.0f}um g{}c{}'.format(self.tempCell.subject, self.tempCell.date,
                                                   self.tempCell.pdepth, self.tempCell.egroup, cluster)
        return objStr
        
